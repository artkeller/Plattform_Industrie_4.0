# Data- und AI-Governance-Rahmenwerk für Zero Trust Architektur (ZTA) mit Künstlicher Intelligenz (KI) in der Industrie 4.0

## Version, Stand und Genehmigung

Dieses Dokument stellt die Version 2.0 des Data- und AI-Governance-Rahmenwerks dar, das am xx. xxx 2026 erstellt wurde. Es wurde von der Geschäftsführung, dem Chief Information Security Officer (CISO) und dem Datenschutzbeauftragten genehmigt, um eine maximale Auditierbarkeit zu gewährleisten. Die Genehmigung erfolgte durch Unterschriften, die in der gedruckten Version dieses Dokuments vorliegen, und dient als Nachweis für die formelle Annahme aller hierin festgelegten Regelungen.

## Rahmenbedingungen und Geltungsbereich

Dieses Governance-Rahmenwerk gilt für alle Systeme der Künstlichen Intelligenz und Komponenten der Zero Trust Architektur, die in der Organisation entwickelt, beschafft, betrieben oder eingesetzt werden. Es umfasst insbesondere Anwendungen in operativen Technologien und Umgebungen der Industrie 4.0, wie etwa in der Produktion, der Lieferkette, der prädiktiven Wartung und der Qualitätskontrolle mit Unterstützung durch Künstliche Intelligenz. Die rechtliche und normative Grundlage dieses Rahmenwerks basiert auf der EU AI Act in der Fassung der Verordnung (EU) 2024/1689, insbesondere den Artikeln 9 bis 15 für Systeme mit hohem Risiko gemäß Annex III, die Use-Cases in kritischer Infrastruktur, Beschäftigung und Produktqualitätssicherung betreffen. Darüber hinaus orientiert es sich an der ISO/IEC 42001:2023 für das Artificial Intelligence Management System, an der ISO/IEC 27001:2022 für das Information Security Management System sowie an der IEC 62443 für die Sicherheit in operativen Technologien. Es integriert die Prinzipien der NIST SP 800-207 für Zero Trust Architecture und der DoD Zero Trust Reference Architecture. Der Geltungsbereich ist auf Systeme mit Relevanz für Künstliche Intelligenz oder Zero Trust Architektur beschränkt, wobei Systeme mit niedrigem Risiko vereinfachte Regelungen unterliegen, die in separaten Anhängen dieses Dokuments detailliert beschrieben werden.

## Ziele und Grundsätze

Die primären Ziele dieses Rahmenwerks bestehen darin, Risiken für die Sicherheit, die Gesundheit, die Grundrechte und die Resilienz in operativen Technologien zu minimieren. Es soll die Nachvollziehbarkeit, die Auditierbarkeit und die Resilienz der Systeme gewährleisten, indem alle regulatorischen Anforderungen der EU AI Act und der ISO 42001 vollständig eingehalten werden. Darüber hinaus fördert es die verantwortungsvolle Nutzung von Künstlicher Intelligenz in dynamischen Umgebungen der Zero Trust Architektur. Die Grundsätze orientieren sich am Alignment mit den Prinzipien der NIST Zero Trust Architecture und der ISO 42001 Annex A. Dazu gehört das Prinzip, niemals blind zu vertrauen, sondern immer zu verifizieren, was durch dynamische und kontextbasierte Authentifizierung umgesetzt wird. Es wird ein Angriff vorausgesetzt, was eine kontinuierliche Überwachung und die Vergabe minimaler Privilegien erfordert. Die Provenienz von Daten und Entscheidungen muss über den gesamten Lebenszyklus hinweg gesichert sein. Eine menschliche Aufsicht ist bei Entscheidungen mit hohem Risiko obligatorisch. Der kontinuierliche Verbesserungsprozess folgt dem Plan-Do-Check-Act-Zyklus, der in allen Prozessen dieses Rahmenwerks verankert ist.

## Klassifizierung von Systemen der Künstlichen Intelligenz

Die risikobasierte Klassifizierung der Systeme der Künstlichen Intelligenz erfolgt gemäß der EU AI Act und der ISO 42001 Annex A.5. In der folgenden Tabelle werden die Risikoklassen detailliert beschrieben, einschließlich der Kriterien, Beispiele aus dem Unternehmen und der daraus resultierenden Anforderungen, die jeweils in vollständigen Sätzen formuliert sind.

| Risikoklasse | Kriterien, die für diese Klasse gelten | Beispiele aus dem Unternehmen, die in diese Klasse fallen | Anforderungen, die für diese Klasse obligatorisch sind |
|--------------|----------------------------------------|----------------------------------------------------------|-------------------------------------------------------|
| Verbotene Systeme | Diese Klasse umfasst Systeme, die gemäß Annex II der EU AI Act verboten sind, wie etwa Social Scoring-Systeme, die Individuen bewerten und diskriminieren. | In unserem Unternehmen existieren keine Systeme, die in diese Klasse fallen, da solche Anwendungen von vornherein ausgeschlossen werden. | Solche Systeme sind nicht zulässig und dürfen weder entwickelt noch eingesetzt werden, um regulatorische Sanktionen zu vermeiden. |
| Systeme mit hohem Risiko | Diese Klasse betrifft Systeme gemäß Annex III der EU AI Act, die eine Sicherheitskomponente darstellen und Auswirkungen auf Gesundheit oder Rechte haben. | Beispiele in unserem Unternehmen sind Systeme der Künstlichen Intelligenz für die Qualitätskontrolle in operativen Technologien oder prädiktive Wartung mit Relevanz für die Sicherheit. | Für diese Systeme müssen die vollständigen Anforderungen der Artikel 9 bis 15 der EU AI Act erfüllt werden, einschließlich einer Conformity Assessment, die durch externe Audits nachgewiesen wird. |
| Systeme mit begrenztem Risiko | Diese Klasse umfasst Systeme, die Transparenzpflichten unterliegen, wie Chatbots oder Systeme zur Erzeugung von Deepfakes. | Beispiele in unserem Unternehmen sind interne Assistenten auf Basis von Künstlicher Intelligenz, die für administrative Zwecke genutzt werden. | Für diese Systeme müssen Transparenzhinweise bereitgestellt werden, die den Nutzern klar mitteilen, dass es sich um Systeme der Künstlichen Intelligenz handelt. |
| Systeme mit minimalem Risiko | Diese Klasse umfasst alle anderen Systeme, die keine der oberen Kriterien erfüllen. | Beispiele in unserem Unternehmen sind interne Automatisierungen ohne Auswirkungen auf Risiken für Personen oder Prozesse. | Für diese Systeme gelten freiwillige Best Practices, die in diesem Rahmenwerk empfohlen werden, um eine konsistente Governance zu gewährleisten. |

Jedes System der Künstlichen Intelligenz durchläuft einen Prozess, in dem eine AI Impact Assessment und eine Fundamental Rights Impact Assessment durchgeführt werden, insbesondere bei Systemen mit hohem Risiko, wie in der ISO 42001 Annex A.5.5 vorgeschrieben.

## Rollen und Verantwortlichkeiten

Die Rollen und Verantwortlichkeiten sind klar definiert, um eine auditierbare Zuweisung zu gewährleisten. In der folgenden Tabelle werden die Rollen beschrieben, einschließlich der Verantwortungen gemäß ISO 42001 Annex A.3.2 und der RACI-Matrix, die Responsible, Accountable, Consulted und Informed darstellt, wobei jede Zelle vollständige Sätze enthält.

| Rolle | Verantwortung, die dieser Rolle zugewiesen ist | RACI-Zuweisung, die für diese Rolle gilt |
|-------|------------------------------------------------|------------------------------------------|
| Geschäftsführung | Die Geschäftsführung ist für die Genehmigung des Rahmenwerks, die Bereitstellung von Ressourcen und die Durchführung von Reviews verantwortlich. | Diese Rolle ist accountable für den gesamten Prozess. |
| AI Governance Board | Das AI Governance Board übernimmt die Strategieentwicklung und die Entscheidungen zu Risiken. | Diese Rolle ist accountable und responsible für strategische Entscheidungen. |
| AI Risk Owner pro System | Der AI Risk Owner ist für das Risikomanagement und die Durchführung von AI Impact Assessments und Fundamental Rights Impact Assessments verantwortlich. | Diese Rolle ist responsible für die operativen Risiken. |
| Chief Information Security Officer oder OT-Security-Verantwortlicher | Der Chief Information Security Officer ist für die Umsetzung der Zero Trust Architektur und die Cybersecurity gemäß Artikel 15 der EU AI Act verantwortlich. | Diese Rolle ist responsible und consulted in Sicherheitsfragen. |
| Datenschutzbeauftragter | Der Datenschutzbeauftragte gewährleistet die Konformität mit der DSGVO, insbesondere hinsichtlich Bias und Datenqualität. | Diese Rolle ist consulted und informed in datenschutzrelevanten Angelegenheiten. |
| AI Ethics Officer | Der AI Ethics Officer führt ethische Bewertungen durch und überwacht die menschliche Aufsicht. | Diese Rolle ist consulted in ethischen Fragen. |
| Fachabteilung oder DevOps-Team | Die Fachabteilung ist für die technische Umsetzung und die Dokumentation verantwortlich. | Diese Rolle ist responsible für die tägliche Implementierung. |

## Menschliche Aufsicht und Eskalationsmechanismen

Die Grundsätze der menschlichen Aufsicht sehen vor, dass immer eine Human-in-the-Loop- oder Human-on-the-Loop-Mechanismus bei Entscheidungen mit hohem Risiko implementiert wird, die Auswirkungen auf Sicherheit oder Personen haben. Eine automatische Eskalation erfolgt bei einem Confidence-Score unter 85 Prozent, einem Kontext-Drift über 10 Prozent in den Daten der Zero Trust Architektur, einer Anomalie im Zustand operativer Prozesse oder einem Bias-Indikator über einem festgelegten Schwellwert. Beispielsweise wird bei einer Entscheidung der Künstlichen Intelligenz in der Qualitätskontrolle eine menschliche Prüfung durchgeführt, wenn die Unsicherheit über 15 Prozent liegt. Bei einer Verletzung der Policy in der Zero Trust Architektur erfolgt eine automatische Blockade und eine Benachrichtigung des Security-Teams, die in Audit-Logs dokumentiert wird.

## Risikomanagement und Impact Assessment

Der Risikomanagement-Prozess folgt dem Plan-Do-Check-Act-Zyklus und umfasst die Identifikation von Risiken für Sicherheit, Rechte und Verfügbarkeit in operativen Technologien, die Bewertung anhand einer Likelihood-Impact-Matrix, die Behandlung durch Controls aus der Annex A der ISO 42001 und das kontinuierliche Monitoring mit jährlichen Reviews. In der folgenden Tabelle wird eine Beispiel-Risiko-Matrix dargestellt, in der jedes Risiko mit Likelihood, Impact, Priorität und mitigierenden Controls beschrieben wird, wobei jede Zelle vollständige Sätze enthält.

| Risiko, das identifiziert wurde | Likelihood, die für dieses Risiko gilt | Impact, der für dieses Risiko erwartet wird | Priorität, die diesem Risiko zugewiesen ist | Mitigierende Controls, die aus der Annex A der ISO 42001 stammen |
|--------------------------------|-----------------------------------------|---------------------------------------------|---------------------------------------------|-----------------------------------------------------------------|
| Bias in Systemen der Künstlichen Intelligenz für operative Qualitätskontrolle | Die Likelihood für dieses Risiko ist mittel, da sie von Datenqualität abhängt. | Der Impact für dieses Risiko ist hoch, da er Diskriminierung verursachen kann. | Die Priorität für dieses Risiko ist hoch aufgrund potenzieller rechtlicher Konsequenzen. | Die mitigierenden Controls umfassen A.7.3 für Bias-Mitigation, A.8.2 für Explainability und A.5.4 für Impact Assessments. |
| Manipulation von Kontext-Daten in der Zero Trust Architektur | Die Likelihood für dieses Risiko ist hoch, da Angriffe in dynamischen Umgebungen häufig sind. | Der Impact für dieses Risiko ist hoch, da er die Systemintegrität beeinträchtigt. | Die Priorität für dieses Risiko ist sehr hoch, da es die Kernsicherheit betrifft. | Die mitigierenden Controls umfassen A.10.1 für Cybersecurity und SR 3.1 aus der IEC 62443 für Gerätesicherheit. |

## Mapping der wesentlichen Controls und Statement of Applicability

Das Statement of Applicability umfasst ein vollständiges Mapping aller 38 Controls der Annex A der ISO 42001. In der folgenden Tabelle werden die Controls detailliert, einschließlich Titel, Anwendung, Umsetzung und Verantwortlichem, wobei jede Zelle vollständige Sätze enthält. (Die Tabelle zeigt einen Auszug; der vollständige Satz befindet sich im Anhang.)

| Annex A Control | Titel des Controls | Anwendung, die für diesen Control gilt | Umsetzung oder Referenzdokument, das diesen Control abdeckt | Verantwortlicher, der für diesen Control zuständig ist |
|-----------------|--------------------|----------------------------------------|------------------------------------------------------------|-------------------------------------------------------|
| A.2.2 | Dieser Control betrifft die AI Policy. | Die Anwendung dieses Controls ist ja, da er für alle Systeme obligatorisch ist. | Die Umsetzung erfolgt durch dieses Dokument, das die Policy definiert. | Der Verantwortliche für diesen Control ist das AI Governance Board. |
| A.2.3 | Dieser Control betrifft das Alignment mit anderen Policies. | Die Anwendung dieses Controls ist ja, da er Integration erfordert. | Die Umsetzung erfolgt durch die ISMS-Policy und die OT-Security-Policy. | Der Verantwortliche für diesen Control ist der Chief Information Security Officer. |
| A.2.4 | Dieser Control betrifft den Review der AI Policy. | Die Anwendung dieses Controls ist ja, da er jährliche Überprüfungen vorschreibt. | Die Umsetzung erfolgt jährlich und bei Change-Triggers durch definierte Prozesse. | Der Verantwortliche für diesen Control ist das AI Governance Board. |
| A.5.1 bis A.5.5 | Diese Controls betreffen Impact Assessments. | Die Anwendung dieser Controls ist ja, da sie für High-Risk-Systeme essenziell sind. | Die Umsetzung erfolgt durch das AI Impact Assessment-Template und die Fundamental Rights Impact Assessment. | Der Verantwortliche für diese Controls ist der AI Risk Owner. |
| A.6.1 bis A.6.2.8 | Diese Controls betreffen AI Lifecycle Controls. | Die Anwendung dieser Controls ist ja, da sie den gesamten Lebenszyklus abdecken. | Die Umsetzung erfolgt durch das Lifecycle-Dokument und die Module ZTA Teil 1 bis 3. | Der Verantwortliche für diese Controls ist das DevOps-Team. |
| A.7.1 bis A.7.5 | Diese Controls betreffen Data for AI Systems. | Die Anwendung dieser Controls ist ja, da Datenqualität zentral ist. | Die Umsetzung erfolgt durch die Data-Provenienz-Policy. | Der Verantwortliche für diese Controls ist der Datenschutzbeauftragte. |
| A.8.1 bis A.8.5 | Diese Controls betreffen Transparency und Explainability. | Die Anwendung dieser Controls ist ja, da Transparenz regulatorisch gefordert ist. | Die Umsetzung erfolgt durch Model Cards und Audit-Logs. | Der Verantwortliche für diese Controls ist der AI Ethics Officer. |
| A.9.1 bis A.9.3 | Diese Controls betreffen Human Oversight. | Die Anwendung dieser Controls ist ja, da menschliche Aufsicht vorgeschrieben ist. | Die Umsetzung erfolgt durch den Abschnitt zur menschlichen Aufsicht und Eskalationsregeln. | Der Verantwortliche für diese Controls ist die Fachabteilung. |
| A.10.1 bis A.10.3 | Diese Controls betreffen Incident und Continuous Improvement. | Die Anwendung dieser Controls ist ja, da kontinuierliche Verbesserung essenziell ist. | Die Umsetzung erfolgt durch Monitoring-KPIs und den Corrective Action Preventive Action-Prozess. | Der Verantwortliche für diese Controls ist der Chief Information Security Officer. |

## Monitoring, Review und Aktualisierung

Der Review-Zyklus dieses Rahmenwerks erfolgt mindestens jährlich sowie bei neuen regulatorischen Anforderungen, signifikanten Änderungen an Systemen der Künstlichen Intelligenz oder Zero Trust Architektur und bei Incidents oder Audit-Findings. Die Key Performance Indicators werden in der folgenden Tabelle detailliert, einschließlich Beschreibung und Zielwert, wobei jede Zelle vollständige Sätze enthält.

| Key Performance Indicator | Beschreibung des Indicators | Zielwert, der für diesen Indicator festgelegt ist |
|---------------------------|-----------------------------|--------------------------------------------------|
| Prozentsatz abgeschlossener AI Impact Assessments bei Systemen mit hohem Risiko | Dieser Indicator misst den Prozentsatz der abgeschlossenen Assessments für Projekte mit hohem Risiko. | Der Zielwert für diesen Indicator beträgt 100 Prozent, um vollständige Compliance zu gewährleisten. |
| Time-to-Detect für Verletzungen in der Zero Trust Architektur | Dieser Indicator misst die Zeit bis zur Erkennung einer Verletzung. | Der Zielwert für diesen Indicator liegt unter 5 Minuten, um schnelle Reaktionen zu ermöglichen. |
| Prozentsatz von Entscheidungen der Künstlichen Intelligenz mit menschlicher Aufsicht bei hohem Risiko | Dieser Indicator misst den Anteil der Entscheidungen, die überwacht werden. | Der Zielwert für diesen Indicator beträgt über 95 Prozent, um Risiken zu minimieren. |

Der kontinuierliche Verbesserungsprozess umfasst den Corrective Action Preventive Action-Prozess, Lessons Learned und Management Reviews gemäß Clause 9.3 der ISO 42001, die in Protokollen dokumentiert werden.

## Anhang: Referenzen und Changelog

Die Referenzen umfassen die EU AI Act unter https://artificialintelligenceact.eu, die ISO/IEC 42001:2023 mit Annex A und 38 Controls, die NIST SP 800-207 sowie die DoD Zero Trust Reference Architecture Version 2.0. Der Changelog dokumentiert Änderungen: In Version 3.0 wurden alle Inhalte zu vollständigen Sätzen und Tabellen erweitert, um maximale Auditierbarkeit und juristische Lesbarkeit zu gewährleisten, einschließlich Evidenz-Nachweisen und formaler Strukturen.
