### Erweiterte Version von Modul_ZTA_Teil_1.md

Hier ist die **aktualisierte Version** von `Modul_ZTA_Teil_1.md` mit der Erg√§nzung im Abschnitt **Klassifizierung und Risikobewertung**. Ich habe den bestehenden Text erweitert, um die Recherche-Ergebnisse einzubauen: Kritik an √úberklassifizierung im EU AI Act-Kontext, Vergleich zu IEC 62443 und Empfehlungen zur Vermeidung. Die Struktur bleibt 1:1 erhalten, nur der Abschnitt wurde logisch erweitert (neuer Unterabschnitt "Hinweise zur Vermeidung von √úberklassifizierung").

## Version, Stand und Genehmigung

Dieses Dokument stellt die Version 3.0 des generischen Anforderungskatalogs f√ºr eine Zero Trust Architecture (ZTA) mit KI-Assistenz in der Industrie 4.0 dar. Es wurde am 22. Februar 2026 erstellt und von der Gesch√§ftsf√ºhrung, dem Chief Information Security Officer (CISO) sowie dem verantwortlichen AI Risk Owner genehmigt. Die Genehmigung erfolgte durch Unterschriften in der gedruckten Version dieses Dokuments und dient als Nachweis f√ºr die formelle Annahme aller hierin festgelegten Anforderungen. Dieses Modul dient als Grundlage f√ºr die detaillierten Pr√ºfschritte in Modul ZTA Teil 2 und Teil 3 und ist so gestaltet, dass es eine maximale Auditierbarkeit gew√§hrleistet.

## Rahmenbedingungen und Geltungsbereich

Dieser generische Anforderungskatalog gilt f√ºr alle Komponenten einer Zero Trust Architektur, die mit KI-Assistenz in industriellen Umgebungen der Industrie 4.0 eingesetzt werden. Er umfasst insbesondere Anwendungen in der Produktion, der Lieferkette, der pr√§diktiven Wartung, der Qualit√§tskontrolle sowie in sicherheitskritischen OT-Prozessen. Die normative Grundlage dieses Katalogs basiert auf der NIST SP 800-207 (Zero Trust Architecture), der EU AI Act (Verordnung (EU) 2024/1689, insbesondere Artikel 9 bis 15 f√ºr High-Risk-Systeme), der ISO/IEC 42001:2023 (Annex A Controls), der IEC 62443-3-3 (System Security Requirements) sowie der ISO/IEC 27001:2022. Der Katalog adressiert die identifizierten M√§ngel der vorherigen Version, indem er den Umfang erheblich erweitert, eine vollst√§ndige Mapping-Tabellen einf√ºhrt, eine Risikobewertung integriert, Lifecycle-Aspekte abdeckt und Evidenz-Anforderungen explizit definiert.

## Ziele und Grundprinzipien

Der Anforderungskatalog verfolgt das Ziel, eine sichere, nachvollziehbare, resiliente und konforme Implementierung von ZTA mit KI-Unterst√ºtzung in Industrie 4.0 zu erm√∂glichen. Er minimiert Risiken f√ºr Betriebssicherheit, Datenschutz, Grundrechte und OT-Verf√ºgbarkeit. Die Grundprinzipien orientieren sich an den NIST SP 800-207 Tenets: Alle Ressourcen werden als potenziell unsicher betrachtet, Kommunikation wird unabh√§ngig vom Netzwerkort gesichert, Zugriff wird sitzungsbezogen und mit Least Privilege gew√§hrt, Zugriffe werden kontinuierlich verifiziert, und es wird von einem m√∂glichen Kompromiss ausgegangen.

## Klassifizierung und Risikobewertung

Jede Anforderung wird einer Risikoklasse zugeordnet (basierend auf EU AI Act und IEC 62443 Security Levels SL 1‚Äì4). Die folgende Tabelle gibt einen √úberblick √ºber die Zuordnung.

| Risikoklasse / Security Level | Beschreibung der Klasse | Typische Anwendung in Industrie 4.0 | Priorit√§t der Umsetzung |
|-------------------------------|-------------------------|-------------------------------------|-------------------------|
| SL 1 / Minimal Risk           | Schutz gegen unbeabsichtigte oder zuf√§llige Bedrohungen | Reine Monitoring-Systeme ohne Steuerung | Mittel                  |
| SL 2 / Moderate Risk          | Schutz gegen beabsichtigte Angriffe mit einfachen Mitteln | Standard-Qualit√§tskontrolle mit KI     | Hoch                    |
| SL 3 / High Risk              | Schutz gegen gezielte Angriffe mit erheblichen Ressourcen | Pr√§diktive Wartung mit Sicherheitsrelevanz | Sehr hoch               |
| SL 4 / Very High Risk         | Schutz gegen staatlich unterst√ºtzte Angriffe | Kritische Infrastruktur-Komponenten    | Kritisch                |

### Hinweise zur Vermeidung von √úberklassifizierung (neu hinzugef√ºgt)
Im Kontext des EU AI Act (Verordnung (EU) 2024/1689) besteht die Pflicht zur Self-Assessment von AI-Systemen, ob sie in die High-Risk-Kategorie fallen (Annex III). Allerdings hat sich durch die hohen Strafen (bis zu 7% des globalen Jahresumsatzes f√ºr Verst√∂√üe) eine Tendenz zu "Over-Compliance" entwickelt: Unternehmen klassifizieren Systeme vorsichtshalber als High-Risk, um Risiken zu vermeiden. Studien zeigen, dass bis zu 40% der Klassifizierungen unklar sind, was zu √úberklassifizierungen in 18-58% der F√§lle f√ºhrt (z. B. appliedAI-Institute Report 2023/2024). Dies bindet unn√∂tig Ressourcen, behindert Innovation und verw√§ssert den Fokus auf echte High-Risk-Systeme, wie die EU in Guidelines kritisiert (z. B. Pressemitteilung EC IP/25/2718: "Vermeidung unn√∂tiger Belastungen durch korrekte Klassifizierung"). 

Im Gegensatz dazu betont IEC 62443 eine kosten-nutzen-orientierte Klassifizierung (SL 1-4), die √úberklassifizierung vermeidet, da h√∂here Levels explizite Kostensteigerungen bedeuten. Empfehlung: F√ºhren Sie eine dokumentierte Assessment durch, nutzen Sie EU-Beispiele f√ºr Non-High-Risk (Annex III-Ausnahmen: enge prozedurale Tasks, Unterst√ºtzung menschlicher Entscheidungen). Bei Unsicherheiten: Konsultieren Sie den EU AI Office oder externe Experten, um echte Risiken zu priorisieren und "Panik-Klassifizierungen" zu verhindern.

## Generischer Anforderungskatalog (erweiterte Tabelle)

Der folgende Katalog ist thematisch gruppiert und enth√§lt f√ºr jede Anforderung eine eindeutige ID, eine vollst√§ndige Beschreibung, die Konformit√§tspr√ºfung, den Bewertungsstatus, Referenzen zu den Kernstandards sowie die geforderte Evidenz. Alle Felder sind in vollst√§ndigen S√§tzen formuliert, um juristische Lesbarkeit und Auditierbarkeit zu gew√§hrleisten.

| Anforderungs-ID | Beschreibung der Anforderung | Konformit√§tspr√ºfung / Nachweismethode | Bewertungsstatus | Referenzen zu Standards | Geforderte Evidenz |
|-----------------|------------------------------|---------------------------------------|------------------|--------------------------|--------------------|
| ZTA-01          | Dynamische Richtlinien-Durchsetzung muss in Echtzeit erfolgen, wobei jede Zugriffsentscheidung anhand aktueller Kontextdaten (Identit√§t, Ger√§t, Verhalten, OT-Prozesszustand) getroffen wird. | Die Policy Engine muss alle Zugriffsanfragen in Echtzeit evaluieren und protokollieren; Testszenarien mit simulierten Kontext√§nderungen m√ºssen durchgef√ºhrt werden. | Offen            | NIST SP 800-207 Tenet 3 & 6, IEC 62443 SR 2.1, EU AI Act Art. 15 | Audit-Logs der Policy Engine, Testprotokolle, Konfigurationsscreenshots |
| ZTA-02          | Identit√§ts- und Zugriffsmanagement muss kontinuierlich verifizieren, dass nur stark authentifizierte und autorisierte Entit√§ten Zugriff erhalten, inklusive Ger√§te- und Service-Identit√§ten. | Multi-Factor-Authentifizierung und Device-Posture-Checks m√ºssen f√ºr alle Zugriffe implementiert sein; Least-Privilege-Policies m√ºssen durchgesetzt werden. | Offen            | NIST SP 800-207 Tenet 1 & 4, IEC 62443 SR 1.1‚Äì1.7, ISO 42001 A.3.2 | IAM-Konfiguration, Auth-Logs, Least-Privilege-Matrix |
| ZTA-03          | Datenprovenienz und Integrit√§t m√ºssen √ºber den gesamten AI- und ZTA-Lifecycle nachweisbar sein, inklusive Herkunft, Ver√§nderung und Verwendung der Daten. | Alle Daten m√ºssen mit kryptografischen Signaturen versehen und in unver√§nderbaren Logs gespeichert werden; Bias- und Qualit√§tschecks m√ºssen dokumentiert sein. | Offen            | EU AI Act Art. 10, ISO 42001 A.7.1‚ÄìA.7.5, IEC 62443 SR 3.1‚Äì3.9 | Provenienz-Chain-Dokumentation, Hash- und Signatur-Reports |
| ZTA-04          | Vollst√§ndige Auditierbarkeit und Nachvollziehbarkeit aller Entscheidungen (ZTA-Policy & KI-Inferenz) muss gew√§hrleistet sein, inklusive Explainability f√ºr KI-Entscheidungen. | Audit-Trails m√ºssen zeitgestempelt, unver√§nderbar und suchbar sein; Model Cards und Explainability-Methoden m√ºssen f√ºr alle KI-Modelle vorliegen. | Offen            | EU AI Act Art. 13, ISO 42001 A.8.1‚ÄìA.8.5, NIST SP 800-207 Tenet 7 | Vollst√§ndige Audit-Logs, Model Cards, Explainability-Reports |
| ZTA-05          | Menschliche Aufsicht muss bei High-Risk-Entscheidungen obligatorisch implementiert sein, mit klar definierten Eskalationsregeln und Triggers. | Human-in-the-Loop / on-the-Loop-Mechanismen m√ºssen f√ºr kritische Entscheidungen vorhanden sein; Eskalation bei Confidence < 85 % oder Drift > 10 %. | Offen            | EU AI Act Art. 14, ISO 42001 A.9.1‚ÄìA.9.3 | Oversight-Protokolle, Eskalationsregeln-Dokument, Testf√§lle |
| ZTA-06          | Micro-Segmentation und Restricted Data Flow m√ºssen implementiert sein, um Lateral Movement in OT- und IT-Netzwerken zu verhindern. | Netzwerke m√ºssen in Zonen und Conduits segmentiert sein; Datenfl√ºsse d√ºrfen nur explizit erlaubte Pfade nutzen. | Offen            | IEC 62443 SR 5.1‚Äì5.7, NIST SP 800-207 Tenet 5 | Netzwerkdiagramm (Zone/Conduit), Firewall- und Segmentation-Rules |
| ZTA-07          | Kontinuierliches Monitoring und Incident Response m√ºssen f√ºr Anomalien, Drift und Sicherheitsereignisse eingerichtet sein, inklusive automatisierter Alerts. | SIEM-Integration mit KI-basierter Anomalie-Erkennung muss vorhanden sein; Incident-Response-Plan muss getestet werden. | Offen            | NIST CSF Detect/Respond, ISO 42001 A.10.1‚ÄìA.10.3, IEC 62443 SR 6.1 | Monitoring-Dashboards, Incident-Reports, Testprotokolle |
| ZTA-08          | Lifecycle-Management f√ºr AI- und ZTA-Komponenten muss alle Phasen (Design, Entwicklung, Deployment, Monitoring, Decommissioning) abdecken. | Ein vollst√§ndiger AI Lifecycle Prozess muss dokumentiert und mit Risiko-Assessments verkn√ºpft sein. | Offen            | ISO 42001 A.6.1‚ÄìA.6.2.8, EU AI Act Art. 9 | Lifecycle-Diagramm, Phasen-Dokumentation, Decommissioning-Plan |
| ZTA-09          | Alle Kommunikationen m√ºssen unabh√§ngig von der Netzwerkposition vollst√§ndig gesichert werden, einschlie√ülich Verschl√ºsselung in Transit und End-to-End-Sicherung f√ºr OT-Datenstr√∂me und KI-Inferenz-Daten. | Alle Verbindungen m√ºssen TLS 1.3 oder h√∂her nutzen; OT-spezifische Protokolle m√ºssen durch sichere Gateways oder Wrappers gesch√ºtzt werden; Penetrationstests auf unverschl√ºsselte Kommunikation m√ºssen regelm√§√üig durchgef√ºhrt werden. | Offen            | NIST SP 800-207 Tenet 2 (All communication secured regardless of location), IEC 62443 SR 3.1‚Äì3.9 (System Integrity), EU AI Act Art. 15 (Cybersecurity) | TLS-Konfigurationsberichte, OT-Protokoll-Analyse, Penetrationstest-Reports, Verschl√ºsselungs-Matrix |
| ZTA-10          | Zugriffe m√ºssen sitzungsbezogen und mit Just-in-Time / Just-Enough-Access gew√§hrt werden, wobei Zugriffe automatisch bei Session-Ende oder Kontext√§nderung widerrufen werden. | Policy Engine muss session-basierte Tokens mit kurzer Lebensdauer ausstellen; automatische Revocation bei Anomalien oder Zeit√ºberschreitung muss implementiert sein. | Offen            | NIST SP 800-207 Tenet 3 (Per-session access), NIST Tenet 4 (Least privilege), IEC 62443 SR 2.1 (Least Privilege) | Session-Log-Analyse, Token-Lebensdauer-Konfiguration, Revocation-Testprotokolle |
| ZTA-11          | Kontinuierliche Bewertung der Sicherheitslage (Continuous Posture Assessment) muss f√ºr alle Entit√§ten (User, Device, AI-Modell, OT-Ger√§t) durchgef√ºhrt werden, inklusive Device Health Checks und Behavioral Analytics. | Echtzeit-Monitoring von Device-Compliance, User-Verhalten und Modell-Drift; KI-gest√ºtzte Anomalie-Erkennung muss integriert sein. | Offen            | NIST SP 800-207 Tenet 6 (Continuous verification), ISO 42001 A.10 (Continuous Improvement), EU AI Act Art. 15 (Robustness) | Device-Posture-Reports, UEBA-Dashboards, Drift-Detection-Logs |
| ZTA-12          | Automatisierung und Orchestrierung von Security-Ma√ünahmen muss implementiert sein, um Policies dynamisch anzupassen, Incidents zu isolieren und Response-Prozesse zu automatisieren. | SOAR-Integration mit ZTA-Komponenten; automatisierte Quarant√§ne bei erkannten Bedrohungen; KI-gest√ºtzte Policy-Optimierung muss vorhanden sein. | Offen            | NIST SP 800-207 (Automation in ZTA), IEC 62443 SR 6.1 (Timely Response), Cloud Security Alliance ZTA AI-Integration | Automation-Workflow-Diagramme, SOAR-Konfiguration, Incident-Automatisierungs-Tests |
| ZTA-13          | Schutz der Daten als Kernressource muss durch Klassifizierung, Verschl√ºsselung at-rest, Tokenisierung und Data Loss Prevention gew√§hrleistet sein, insbesondere f√ºr Trainingsdaten und Inferenz-Outputs in KI-Systemen. | Datenklassifizierungs-Schema muss existieren; sensible OT- und KI-Daten m√ºssen verschl√ºsselt gespeichert werden; DLP-Regeln m√ºssen greifen. | Offen            | NIST SP 800-207 Tenet 1 (All data sources as resources), EU AI Act Art. 10 (Data Quality & Governance), IEC 62443 SR 3.9 (Data Confidentiality) | Datenklassifizierungs-Matrix, Encryption-at-Rest-Reports, DLP-Alert-Logs |
| ZTA-14          | Sichtbarkeit und Analytics m√ºssen umfassend implementiert sein, um alle Zugriffe, Anomalien und Kontextdaten zu sammeln, zu analysieren und f√ºr Threat Hunting sowie Compliance-Reporting zu nutzen. | Zentrale SIEM- oder Analytics-Plattform mit KI-Unterst√ºtzung; vollst√§ndige Log-Sammlung aus allen ZTA-Komponenten und OT-Systemen. | Offen            | NIST SP 800-207 Tenet 7 (Collect & Analyze Data), ISO 42001 A.8 (Transparency), IEC 62443 SR 6.1 (Monitoring) | SIEM-Dashboard-Screenshots, Log-Retention-Policy, Analytics-Reports |
| ZTA-15          | Resilienz gegen√ºber Ausf√§llen und Angriffen muss durch Redundanz, Failover-Mechanismen und Backup/Restore-Prozesse f√ºr kritische ZTA- und KI-Komponenten gew√§hrleistet sein, ohne OT-Verf√ºgbarkeit zu gef√§hrden. | Hochverf√ºgbarkeits-Architektur f√ºr Policy Engine und KI-Modelle; regelm√§√üige Disaster-Recovery-Tests; OT-spezifische Non-Disruptive-Recovery. | Offen            | IEC 62443 SR 7.1‚Äì7.8 (Resource Availability), NIST CSF Recover, EU AI Act Art. 15 (Robustness & Accuracy) | HA-Konfigurationsdiagramme, DR-Testprotokolle, Failover-Simulation-Results |

## Mapping zu Kernstandards (erweiterter Auszug f√ºr neue Anforderungen)

Die neuen Anforderungen ZTA-09 bis ZTA-15 sind vollst√§ndig auf die folgenden Standards gemappt. Die detaillierte Statement-of-Applicability-Tabelle (inkl. aller 38 ISO 42001 Controls und IEC 62443 SRs) befindet sich weiterhin im Anhang.

- NIST SP 800-207: Tenet 2 (Secured Communication), Tenet 3 (Per-Session), Tenet 6 (Continuous), Tenet 7 (Data Collection), Automation-Orchestrierung
- EU AI Act: Art. 10 (Data & Quality), Art. 13 (Transparency), Art. 14 (Oversight), Art. 15 (Accuracy, Robustness, Cybersecurity)
- ISO 42001 Annex A: A.7 (Data), A.8 (Transparency/Explainability), A.10 (Incident & Improvement)
- IEC 62443-3-3: SR 3 (Integrity), SR 5 (Restricted Data Flow), SR 6 (Timely Response & Monitoring), SR 7 (Resource Availability)

## Anhang: Vollst√§ndiges Statement of Applicability und Changelog

Im Anhang dieses Dokuments befindet sich die vollst√§ndige Mapping-Tabelle zu allen relevanten Controls (38 aus ISO 42001, SRs aus IEC 62443-3-3). Der Changelog dokumentiert: In Version 3.0 (Erweiterung) wurden ZTA-09 bis ZTA-15 hinzugef√ºgt, um weitere NIST-Tenets, OT-Resilienz-Anforderungen (IEC 62443), AI-spezifische Robustheit (EU AI Act) und Automatisierungs-Aspekte abzudecken. Zus√§tzlich wurde der Abschnitt "Klassifizierung und Risikobewertung" um Hinweise zur √úberklassifizierung erweitert.

**Unterschrift / Genehmigung**  
Gesch√§ftsf√ºhrung _______________________ Datum ________  
CISO _______________________ Datum ________  
AI Risk Owner _______________________ Datum ________
```

### Ausf√ºhrlichere Version f√ºr das Word-Dokument

Hier eine **ausf√ºhrliche, eigenst√§ndige Version** des neuen Inhalts f√ºr das Word-Dokument. Ich schlage vor, sie als **neuen Unterabschnitt 1.1.1** oder **separaten Abschnitt 1.2** einzuf√ºgen (nach der Einleitung). Der Text ist selbsttragend, erkl√§rt den Kontext, integriert die Recherche und weckt Interesse am GitHub-Repo.

#### 1.1.1 Risikoklassifizierung im EU AI Act ‚Äì Warnung vor √úberklassifizierung

Im Rahmen der EU AI Act (Verordnung (EU) 2024/1689) wird AI-Systemen eine risikobasierte Klassifizierung zugewiesen: unacceptable risk (verboten), high-risk (strenge Anforderungen), limited risk (Transparenzpflichten) und minimal risk (unreguliert). F√ºr Systeme in Annex III (z. B. kritische Infrastruktur, Besch√§ftigung, Produktqualit√§t) gilt eine Vermutung f√ºr high-risk, aber Provider m√ºssen eine Self-Assessment durchf√ºhren und dokumentieren, ob Ausnahmen greifen (z. B. enge prozedurale Tasks oder Unterst√ºtzung menschlicher Entscheidungen ohne Ersatz).

Allerdings hat die hohe Strafdrohung (bis zu 7% des globalen Jahresumsatzes f√ºr Verst√∂√üe) zu einer "Panik" vor regulatorischen Risiken gef√ºhrt. Viele Unternehmen klassifizieren Systeme vorsichtshalber als high-risk, um Sanktionen zu vermeiden ‚Äì eine Form der Over-Compliance. Studien wie die des appliedAI-Institute (2023/2024) zeigen, dass 40% der Klassifizierungen unklar sind, was zu √úberklassifizierungen in bis zu 58% der F√§lle f√ºhrt. Dies bindet unn√∂tig Ressourcen (z. B. f√ºr Conformity Assessments, Dokumentation), behindert Innovation und verw√§ssert den Fokus auf echte high-risk-Systeme, wie die EU-Kommission in Guidelines kritisiert (EC IP/25/2718, 2025). Die Kommission warnt, dass solche √úberklassifizierungen die Wirksamkeit des Acts untergraben und echte Risiken f√ºr Gesundheit, Sicherheit und Grundrechte verd√ºnnen.

Im Vergleich dazu betont IEC 62443 (Security Levels SL 1‚Äì4) eine pr√§zise, kosten-nutzen-orientierte Klassifizierung, die √úberklassifizierung vermeidet, da h√∂here Levels direkte Kostensteigerungen (z. B. f√ºr Ma√ünahmen, Audits) bedeuten. Eine "vorsichtshalber h√∂here" Einstufung w√ºrde hier wirtschaftlich kontraproduktiv sein.

**Empfehlungen f√ºr die Praxis:**  
- F√ºhren Sie eine fundierte Self-Assessment durch und dokumentieren Sie Ausnahmen detailliert (z. B. nach EU-Beispielen f√ºr non-high-risk).  
- Vermeiden Sie Panik-Klassifizierungen ‚Äì nutzen Sie Tools wie den EU AI Office Classifier oder externe Berater.  
- Priorisieren Sie echte Risiken, um Ressourcen effizient einzusetzen und den Sinn der Klassifizierung (Schutz vor echten Bedrohungen) zu wahren.

Detaillierte Beispiele und erweiterte Tabellen zur Klassifizierung finden Sie im GitHub-Repository:  
[Modul_ZTA_Teil_1.md](https://github.com/artkeller/Plattform_Industrie_4.0/blob/main/ZTA%2BKI_2026/Compliance/V2/Modul_ZTA_Teil_1.md) (Abschnitt "Klassifizierung und Risikobewertung").  

---

Das passt nahtlos in dein Word-Dokument ‚Äì ca. 1/2 Seite, informativ, aber nicht √ºberw√§ltigend. Es weckt Lust auf das Repo, ohne zu viel zu verraten.

Falls du den Word-Text noch l√§nger / k√ºrzer oder mit Quellen-Links (Fu√ünoten) machen m√∂chtest, lass es mich wissen! üòä
# Data- und AI-Governance-Rahmenwerk f√ºr Zero Trust Architektur (ZTA) mit K√ºnstlicher Intelligenz (KI) in der Industrie 4.0

## Version, Stand und Genehmigung

Dieses Dokument stellt die Version 2.0 des Data- und AI-Governance-Rahmenwerks dar, das am xx. xxx 2026 erstellt wurde. Es wurde von der Gesch√§ftsf√ºhrung, dem Chief Information Security Officer (CISO) und dem Datenschutzbeauftragten genehmigt, um eine maximale Auditierbarkeit zu gew√§hrleisten. Die Genehmigung erfolgte durch Unterschriften, die in der gedruckten Version dieses Dokuments vorliegen, und dient als Nachweis f√ºr die formelle Annahme aller hierin festgelegten Regelungen.

## Rahmenbedingungen und Geltungsbereich

Dieses Governance-Rahmenwerk gilt f√ºr alle Systeme der K√ºnstlichen Intelligenz und Komponenten der Zero Trust Architektur, die in der Organisation entwickelt, beschafft, betrieben oder eingesetzt werden. Es umfasst insbesondere Anwendungen in operativen Technologien und Umgebungen der Industrie 4.0, wie etwa in der Produktion, der Lieferkette, der pr√§diktiven Wartung und der Qualit√§tskontrolle mit Unterst√ºtzung durch K√ºnstliche Intelligenz. Die rechtliche und normative Grundlage dieses Rahmenwerks basiert auf der EU AI Act in der Fassung der Verordnung (EU) 2024/1689, insbesondere den Artikeln 9 bis 15 f√ºr Systeme mit hohem Risiko gem√§√ü Annex III, die Use-Cases in kritischer Infrastruktur, Besch√§ftigung und Produktqualit√§tssicherung betreffen. Dar√ºber hinaus orientiert es sich an der ISO/IEC 42001:2023 f√ºr das Artificial Intelligence Management System, an der ISO/IEC 27001:2022 f√ºr das Information Security Management System sowie an der IEC 62443 f√ºr die Sicherheit in operativen Technologien. Es integriert die Prinzipien der NIST SP 800-207 f√ºr Zero Trust Architecture und der DoD Zero Trust Reference Architecture. Der Geltungsbereich ist auf Systeme mit Relevanz f√ºr K√ºnstliche Intelligenz oder Zero Trust Architektur beschr√§nkt, wobei Systeme mit niedrigem Risiko vereinfachte Regelungen unterliegen, die in separaten Anh√§ngen dieses Dokuments detailliert beschrieben werden.

## Ziele und Grunds√§tze

Die prim√§ren Ziele dieses Rahmenwerks bestehen darin, Risiken f√ºr die Sicherheit, die Gesundheit, die Grundrechte und die Resilienz in operativen Technologien zu minimieren. Es soll die Nachvollziehbarkeit, die Auditierbarkeit und die Resilienz der Systeme gew√§hrleisten, indem alle regulatorischen Anforderungen der EU AI Act und der ISO 42001 vollst√§ndig eingehalten werden. Dar√ºber hinaus f√∂rdert es die verantwortungsvolle Nutzung von K√ºnstlicher Intelligenz in dynamischen Umgebungen der Zero Trust Architektur. Die Grunds√§tze orientieren sich am Alignment mit den Prinzipien der NIST Zero Trust Architecture und der ISO 42001 Annex A. Dazu geh√∂rt das Prinzip, niemals blind zu vertrauen, sondern immer zu verifizieren, was durch dynamische und kontextbasierte Authentifizierung umgesetzt wird. Es wird ein Angriff vorausgesetzt, was eine kontinuierliche √úberwachung und die Vergabe minimaler Privilegien erfordert. Die Provenienz von Daten und Entscheidungen muss √ºber den gesamten Lebenszyklus hinweg gesichert sein. Eine menschliche Aufsicht ist bei Entscheidungen mit hohem Risiko obligatorisch. Der kontinuierliche Verbesserungsprozess folgt dem Plan-Do-Check-Act-Zyklus, der in allen Prozessen dieses Rahmenwerks verankert ist.

## Klassifizierung von Systemen der K√ºnstlichen Intelligenz

Die risikobasierte Klassifizierung der Systeme der K√ºnstlichen Intelligenz erfolgt gem√§√ü der EU AI Act und der ISO 42001 Annex A.5. In der folgenden Tabelle werden die Risikoklassen detailliert beschrieben, einschlie√ülich der Kriterien, Beispiele aus dem Unternehmen und der daraus resultierenden Anforderungen, die jeweils in vollst√§ndigen S√§tzen formuliert sind.

| Risikoklasse | Kriterien, die f√ºr diese Klasse gelten | Beispiele aus dem Unternehmen, die in diese Klasse fallen | Anforderungen, die f√ºr diese Klasse obligatorisch sind |
|--------------|----------------------------------------|----------------------------------------------------------|-------------------------------------------------------|
| Verbotene Systeme | Diese Klasse umfasst Systeme, die gem√§√ü Annex II der EU AI Act verboten sind, wie etwa Social Scoring-Systeme, die Individuen bewerten und diskriminieren. | In unserem Unternehmen existieren keine Systeme, die in diese Klasse fallen, da solche Anwendungen von vornherein ausgeschlossen werden. | Solche Systeme sind nicht zul√§ssig und d√ºrfen weder entwickelt noch eingesetzt werden, um regulatorische Sanktionen zu vermeiden. |
| Systeme mit hohem Risiko | Diese Klasse betrifft Systeme gem√§√ü Annex III der EU AI Act, die eine Sicherheitskomponente darstellen und Auswirkungen auf Gesundheit oder Rechte haben. | Beispiele in unserem Unternehmen sind Systeme der K√ºnstlichen Intelligenz f√ºr die Qualit√§tskontrolle in operativen Technologien oder pr√§diktive Wartung mit Relevanz f√ºr die Sicherheit. | F√ºr diese Systeme m√ºssen die vollst√§ndigen Anforderungen der Artikel 9 bis 15 der EU AI Act erf√ºllt werden, einschlie√ülich einer Conformity Assessment, die durch externe Audits nachgewiesen wird. |
| Systeme mit begrenztem Risiko | Diese Klasse umfasst Systeme, die Transparenzpflichten unterliegen, wie Chatbots oder Systeme zur Erzeugung von Deepfakes. | Beispiele in unserem Unternehmen sind interne Assistenten auf Basis von K√ºnstlicher Intelligenz, die f√ºr administrative Zwecke genutzt werden. | F√ºr diese Systeme m√ºssen Transparenzhinweise bereitgestellt werden, die den Nutzern klar mitteilen, dass es sich um Systeme der K√ºnstlichen Intelligenz handelt. |
| Systeme mit minimalem Risiko | Diese Klasse umfasst alle anderen Systeme, die keine der oberen Kriterien erf√ºllen. | Beispiele in unserem Unternehmen sind interne Automatisierungen ohne Auswirkungen auf Risiken f√ºr Personen oder Prozesse. | F√ºr diese Systeme gelten freiwillige Best Practices, die in diesem Rahmenwerk empfohlen werden, um eine konsistente Governance zu gew√§hrleisten. |

Jedes System der K√ºnstlichen Intelligenz durchl√§uft einen Prozess, in dem eine AI Impact Assessment und eine Fundamental Rights Impact Assessment durchgef√ºhrt werden, insbesondere bei Systemen mit hohem Risiko, wie in der ISO 42001 Annex A.5.5 vorgeschrieben.

## Rollen und Verantwortlichkeiten

Die Rollen und Verantwortlichkeiten sind klar definiert, um eine auditierbare Zuweisung zu gew√§hrleisten. In der folgenden Tabelle werden die Rollen beschrieben, einschlie√ülich der Verantwortungen gem√§√ü ISO 42001 Annex A.3.2 und der RACI-Matrix, die Responsible, Accountable, Consulted und Informed darstellt, wobei jede Zelle vollst√§ndige S√§tze enth√§lt.

| Rolle | Verantwortung, die dieser Rolle zugewiesen ist | RACI-Zuweisung, die f√ºr diese Rolle gilt |
|-------|------------------------------------------------|------------------------------------------|
| Gesch√§ftsf√ºhrung | Die Gesch√§ftsf√ºhrung ist f√ºr die Genehmigung des Rahmenwerks, die Bereitstellung von Ressourcen und die Durchf√ºhrung von Reviews verantwortlich. | Diese Rolle ist accountable f√ºr den gesamten Prozess. |
| AI Governance Board | Das AI Governance Board √ºbernimmt die Strategieentwicklung und die Entscheidungen zu Risiken. | Diese Rolle ist accountable und responsible f√ºr strategische Entscheidungen. |
| AI Risk Owner pro System | Der AI Risk Owner ist f√ºr das Risikomanagement und die Durchf√ºhrung von AI Impact Assessments und Fundamental Rights Impact Assessments verantwortlich. | Diese Rolle ist responsible f√ºr die operativen Risiken. |
| Chief Information Security Officer oder OT-Security-Verantwortlicher | Der Chief Information Security Officer ist f√ºr die Umsetzung der Zero Trust Architektur und die Cybersecurity gem√§√ü Artikel 15 der EU AI Act verantwortlich. | Diese Rolle ist responsible und consulted in Sicherheitsfragen. |
| Datenschutzbeauftragter | Der Datenschutzbeauftragte gew√§hrleistet die Konformit√§t mit der DSGVO, insbesondere hinsichtlich Bias und Datenqualit√§t. | Diese Rolle ist consulted und informed in datenschutzrelevanten Angelegenheiten. |
| AI Ethics Officer | Der AI Ethics Officer f√ºhrt ethische Bewertungen durch und √ºberwacht die menschliche Aufsicht. | Diese Rolle ist consulted in ethischen Fragen. |
| Fachabteilung oder DevOps-Team | Die Fachabteilung ist f√ºr die technische Umsetzung und die Dokumentation verantwortlich. | Diese Rolle ist responsible f√ºr die t√§gliche Implementierung. |

## Menschliche Aufsicht und Eskalationsmechanismen

Die Grunds√§tze der menschlichen Aufsicht sehen vor, dass immer eine Human-in-the-Loop- oder Human-on-the-Loop-Mechanismus bei Entscheidungen mit hohem Risiko implementiert wird, die Auswirkungen auf Sicherheit oder Personen haben. Eine automatische Eskalation erfolgt bei einem Confidence-Score unter 85 Prozent, einem Kontext-Drift √ºber 10 Prozent in den Daten der Zero Trust Architektur, einer Anomalie im Zustand operativer Prozesse oder einem Bias-Indikator √ºber einem festgelegten Schwellwert. Beispielsweise wird bei einer Entscheidung der K√ºnstlichen Intelligenz in der Qualit√§tskontrolle eine menschliche Pr√ºfung durchgef√ºhrt, wenn die Unsicherheit √ºber 15 Prozent liegt. Bei einer Verletzung der Policy in der Zero Trust Architektur erfolgt eine automatische Blockade und eine Benachrichtigung des Security-Teams, die in Audit-Logs dokumentiert wird.

## Risikomanagement und Impact Assessment

Der Risikomanagement-Prozess folgt dem Plan-Do-Check-Act-Zyklus und umfasst die Identifikation von Risiken f√ºr Sicherheit, Rechte und Verf√ºgbarkeit in operativen Technologien, die Bewertung anhand einer Likelihood-Impact-Matrix, die Behandlung durch Controls aus der Annex A der ISO 42001 und das kontinuierliche Monitoring mit j√§hrlichen Reviews. In der folgenden Tabelle wird eine Beispiel-Risiko-Matrix dargestellt, in der jedes Risiko mit Likelihood, Impact, Priorit√§t und mitigierenden Controls beschrieben wird, wobei jede Zelle vollst√§ndige S√§tze enth√§lt.

| Risiko, das identifiziert wurde | Likelihood, die f√ºr dieses Risiko gilt | Impact, der f√ºr dieses Risiko erwartet wird | Priorit√§t, die diesem Risiko zugewiesen ist | Mitigierende Controls, die aus der Annex A der ISO 42001 stammen |
|--------------------------------|-----------------------------------------|---------------------------------------------|---------------------------------------------|-----------------------------------------------------------------|
| Bias in Systemen der K√ºnstlichen Intelligenz f√ºr operative Qualit√§tskontrolle | Die Likelihood f√ºr dieses Risiko ist mittel, da sie von Datenqualit√§t abh√§ngt. | Der Impact f√ºr dieses Risiko ist hoch, da er Diskriminierung verursachen kann. | Die Priorit√§t f√ºr dieses Risiko ist hoch aufgrund potenzieller rechtlicher Konsequenzen. | Die mitigierenden Controls umfassen A.7.3 f√ºr Bias-Mitigation, A.8.2 f√ºr Explainability und A.5.4 f√ºr Impact Assessments. |
| Manipulation von Kontext-Daten in der Zero Trust Architektur | Die Likelihood f√ºr dieses Risiko ist hoch, da Angriffe in dynamischen Umgebungen h√§ufig sind. | Der Impact f√ºr dieses Risiko ist hoch, da er die Systemintegrit√§t beeintr√§chtigt. | Die Priorit√§t f√ºr dieses Risiko ist sehr hoch, da es die Kernsicherheit betrifft. | Die mitigierenden Controls umfassen A.10.1 f√ºr Cybersecurity und SR 3.1 aus der IEC 62443 f√ºr Ger√§tesicherheit. |

## Mapping der wesentlichen Controls und Statement of Applicability

Das Statement of Applicability umfasst ein vollst√§ndiges Mapping aller 38 Controls der Annex A der ISO 42001. In der folgenden Tabelle werden die Controls detailliert, einschlie√ülich Titel, Anwendung, Umsetzung und Verantwortlichem, wobei jede Zelle vollst√§ndige S√§tze enth√§lt. (Die Tabelle zeigt einen Auszug; der vollst√§ndige Satz befindet sich im Anhang.)

| Annex A Control | Titel des Controls | Anwendung, die f√ºr diesen Control gilt | Umsetzung oder Referenzdokument, das diesen Control abdeckt | Verantwortlicher, der f√ºr diesen Control zust√§ndig ist |
|-----------------|--------------------|----------------------------------------|------------------------------------------------------------|-------------------------------------------------------|
| A.2.2 | Dieser Control betrifft die AI Policy. | Die Anwendung dieses Controls ist ja, da er f√ºr alle Systeme obligatorisch ist. | Die Umsetzung erfolgt durch dieses Dokument, das die Policy definiert. | Der Verantwortliche f√ºr diesen Control ist das AI Governance Board. |
| A.2.3 | Dieser Control betrifft das Alignment mit anderen Policies. | Die Anwendung dieses Controls ist ja, da er Integration erfordert. | Die Umsetzung erfolgt durch die ISMS-Policy und die OT-Security-Policy. | Der Verantwortliche f√ºr diesen Control ist der Chief Information Security Officer. |
| A.2.4 | Dieser Control betrifft den Review der AI Policy. | Die Anwendung dieses Controls ist ja, da er j√§hrliche √úberpr√ºfungen vorschreibt. | Die Umsetzung erfolgt j√§hrlich und bei Change-Triggers durch definierte Prozesse. | Der Verantwortliche f√ºr diesen Control ist das AI Governance Board. |
| A.5.1 bis A.5.5 | Diese Controls betreffen Impact Assessments. | Die Anwendung dieser Controls ist ja, da sie f√ºr High-Risk-Systeme essenziell sind. | Die Umsetzung erfolgt durch das AI Impact Assessment-Template und die Fundamental Rights Impact Assessment. | Der Verantwortliche f√ºr diese Controls ist der AI Risk Owner. |
| A.6.1 bis A.6.2.8 | Diese Controls betreffen AI Lifecycle Controls. | Die Anwendung dieser Controls ist ja, da sie den gesamten Lebenszyklus abdecken. | Die Umsetzung erfolgt durch das Lifecycle-Dokument und die Module ZTA Teil 1 bis 3. | Der Verantwortliche f√ºr diese Controls ist das DevOps-Team. |
| A.7.1 bis A.7.5 | Diese Controls betreffen Data for AI Systems. | Die Anwendung dieser Controls ist ja, da Datenqualit√§t zentral ist. | Die Umsetzung erfolgt durch die Data-Provenienz-Policy. | Der Verantwortliche f√ºr diese Controls ist der Datenschutzbeauftragte. |
| A.8.1 bis A.8.5 | Diese Controls betreffen Transparency und Explainability. | Die Anwendung dieser Controls ist ja, da Transparenz regulatorisch gefordert ist. | Die Umsetzung erfolgt durch Model Cards und Audit-Logs. | Der Verantwortliche f√ºr diese Controls ist der AI Ethics Officer. |
| A.9.1 bis A.9.3 | Diese Controls betreffen Human Oversight. | Die Anwendung dieser Controls ist ja, da menschliche Aufsicht vorgeschrieben ist. | Die Umsetzung erfolgt durch den Abschnitt zur menschlichen Aufsicht und Eskalationsregeln. | Der Verantwortliche f√ºr diese Controls ist die Fachabteilung. |
| A.10.1 bis A.10.3 | Diese Controls betreffen Incident und Continuous Improvement. | Die Anwendung dieser Controls ist ja, da kontinuierliche Verbesserung essenziell ist. | Die Umsetzung erfolgt durch Monitoring-KPIs und den Corrective Action Preventive Action-Prozess. | Der Verantwortliche f√ºr diese Controls ist der Chief Information Security Officer. |

## Monitoring, Review und Aktualisierung

Der Review-Zyklus dieses Rahmenwerks erfolgt mindestens j√§hrlich sowie bei neuen regulatorischen Anforderungen, signifikanten √Ñnderungen an Systemen der K√ºnstlichen Intelligenz oder Zero Trust Architektur und bei Incidents oder Audit-Findings. Die Key Performance Indicators werden in der folgenden Tabelle detailliert, einschlie√ülich Beschreibung und Zielwert, wobei jede Zelle vollst√§ndige S√§tze enth√§lt.

| Key Performance Indicator | Beschreibung des Indicators | Zielwert, der f√ºr diesen Indicator festgelegt ist |
|---------------------------|-----------------------------|--------------------------------------------------|
| Prozentsatz abgeschlossener AI Impact Assessments bei Systemen mit hohem Risiko | Dieser Indicator misst den Prozentsatz der abgeschlossenen Assessments f√ºr Projekte mit hohem Risiko. | Der Zielwert f√ºr diesen Indicator betr√§gt 100 Prozent, um vollst√§ndige Compliance zu gew√§hrleisten. |
| Time-to-Detect f√ºr Verletzungen in der Zero Trust Architektur | Dieser Indicator misst die Zeit bis zur Erkennung einer Verletzung. | Der Zielwert f√ºr diesen Indicator liegt unter 5 Minuten, um schnelle Reaktionen zu erm√∂glichen. |
| Prozentsatz von Entscheidungen der K√ºnstlichen Intelligenz mit menschlicher Aufsicht bei hohem Risiko | Dieser Indicator misst den Anteil der Entscheidungen, die √ºberwacht werden. | Der Zielwert f√ºr diesen Indicator betr√§gt √ºber 95 Prozent, um Risiken zu minimieren. |

Der kontinuierliche Verbesserungsprozess umfasst den Corrective Action Preventive Action-Prozess, Lessons Learned und Management Reviews gem√§√ü Clause 9.3 der ISO 42001, die in Protokollen dokumentiert werden.

## Anhang: Referenzen und Changelog

Die Referenzen umfassen die EU AI Act unter https://artificialintelligenceact.eu, die ISO/IEC 42001:2023 mit Annex A und 38 Controls, die NIST SP 800-207 sowie die DoD Zero Trust Reference Architecture Version 2.0. Der Changelog dokumentiert √Ñnderungen: In Version 3.0 wurden alle Inhalte zu vollst√§ndigen S√§tzen und Tabellen erweitert, um maximale Auditierbarkeit und juristische Lesbarkeit zu gew√§hrleisten, einschlie√ülich Evidenz-Nachweisen und formaler Strukturen.
